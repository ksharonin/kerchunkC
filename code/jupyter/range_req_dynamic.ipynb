{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# s3 range-get requests w/ AWS SDK\n",
    "\n",
    "Try to manually execute some S3 range-get requests \n",
    "using lower-level Python code (e.g., Python AWS SDK — \n",
    "I think this is the boto library) and decompress and \n",
    "decode them into numpy arrays using the information in the \n",
    "Kerchunk reference files (byte order, dimensionality, etc.). \n",
    "This will help you understand the steps youll eventually need to \n",
    "do in C++, and help you make sure what youre doing is actually possible \n",
    "(e.g., there arent weird web issues that arent your fault) in a \n",
    "friendlier interactive environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import zlib\n",
    "import json\n",
    "import boto3\n",
    "import boto\n",
    "import fsspec\n",
    "import fsspec.utils\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import os\n",
    "import ujson\n",
    "import s3fs\n",
    "import numcodecs\n",
    "\n",
    "import kerchunk.combine\n",
    "from kerchunk.zarr import single_zarr\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream sample bytes from generic NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x89HDF\\r\\n\\x1a\\n\\x00\\x00\\x00\\x00\\x00\\x08\\x08\\x00\\x04\\x00\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff'\n",
      "1000100101001000010001000100011000001101\n"
     ]
    }
   ],
   "source": [
    "# url: 's3://era5-pds/2020/*/data/air_pressure_at_mean_sea_level.nc'[:2]\n",
    "# replace with 01 etc prefixes \n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3_url = 's3://era5-pds/2020/01/data/air_pressure_at_mean_sea_level.nc'\n",
    "byte_range = 'bytes=0-64'\n",
    "\n",
    "s3_parts = s3_url.split(\"/\")\n",
    "bucket_name = s3_parts[2]\n",
    "object_key = \"/\".join(s3_parts[3:])\n",
    "\n",
    "response = s3.get_object(Bucket=bucket_name, Key=object_key, Range=byte_range)\n",
    "\n",
    "# Read and print the content of the specified byte range\n",
    "content = response['Body'].read()\n",
    "\n",
    "print(content[:40])\n",
    "binary_string = \"{:08b}\".format(int(content.hex(),16))\n",
    "print(binary_string[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now directly apply metadata to extract a Zarr chunk\n",
    "\n",
    "#### Zarr storage sec:\n",
    "Multiple arrays can be stored in the same array store by \n",
    "associating each array with a different logical path.\n",
    "A logical path is simply an ASCII string. The logical \n",
    "path is used to form a prefix for keys used by the array. \n",
    "For example, if an array is stored at logical path “foo/bar” \n",
    "then the array metadata will be stored under the key “foo/bar/.zarray”,\n",
    "the user-defined attributes will be stored under the key “foo/bar/.zattrs”, \n",
    "and the chunks will be stored under keys like “foo/bar/0.0”, “foo/bar/0.1”, etc.\n",
    "\n",
    "The compressed sequence of bytes for each chunk is stored \n",
    "under a key formed from the index of the chunk within \n",
    "the grid of chunks representing the array. To form a string key \n",
    "for a chunk, the indices are converted to strings and concatenated\n",
    "with the period character (“.”) separating each index. For example, \n",
    "given an array with shape (10000, 10000) and chunk shape (1000, 1000) \n",
    "there will be 100 chunks laid out in a 10 by 10 grid. \n",
    "The chunk with indices (0, 0) provides data for rows 0-999 and columns 0-999 \n",
    "and is stored under the key “0.0”; the chunk with indices (2, 4) provides \n",
    "data for rows 2000-2999 and columns 4000-4999 and is stored under the key “2.4”; etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_binary(data):\n",
    "    binary_string = ' '.join(format(byte, '08b') for byte in data)\n",
    "    return binary_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata display\n",
      "dict_keys(['version', 'refs'])\n",
      "sample dict values\n",
      "('.zgroup', '{\"zarr_format\":2}')\n",
      "('.zattrs', '{\"institution\":\"ECMWF\",\"source\":\"Reanalysis\",\"title\":\"ERA5 forecasts\"}')\n",
      "('air_pressure_at_mean_sea_level/.zarray', '{\"chunks\":[24,100,100],\"compressor\":{\"id\":\"zlib\",\"level\":4},\"dtype\":\"<f4\",\"fill_value\":9.969209968386869e+36,\"filters\":[{\"elementsize\":4,\"id\":\"shuffle\"}],\"order\":\"C\",\"shape\":[744,721,1440],\"zarr_format\":2}')\n",
      "('air_pressure_at_mean_sea_level/.zattrs', '{\"_ARRAY_DIMENSIONS\":[\"time0\",\"lat\",\"lon\"],\"least_significant_digit\":2,\"long_name\":\"Mean sea level pressure\",\"nameCDM\":\"Mean_sea_level_pressure_surface\",\"nameECMWF\":\"Mean sea level pressure\",\"product_type\":\"analysis\",\"shortNameECMWF\":\"msl\",\"standard_name\":\"air_pressure_at_mean_sea_level\",\"units\":\"Pa\"}')\n",
      "('air_pressure_at_mean_sea_level/0.0.0', ['era5-pds/2020/01/data/air_pressure_at_mean_sea_level.nc', 19226, 256358])\n",
      "('air_pressure_at_mean_sea_level/0.0.1', ['era5-pds/2020/01/data/air_pressure_at_mean_sea_level.nc', 275584, 256295])\n",
      "{\"chunks\":[24,100,100],\"compressor\":{\"id\":\"zlib\",\"level\":4},\"dtype\":\"<f4\",\"fill_value\":9.969209968386869e+36,\"filters\":[{\"elementsize\":4,\"id\":\"shuffle\"}],\"order\":\"C\",\"shape\":[744,721,1440],\"zarr_format\":2}\n",
      "['era5-pds/2020/01/data/air_pressure_at_mean_sea_level.nc', 19226, 256358]\n",
      "['era5-pds/2020/01/data/air_pressure_at_mean_sea_level.nc', 275584, 256295]\n",
      "['era5-pds/2020/01/data/air_pressure_at_mean_sea_level.nc', 531879, 256462]\n",
      "\n",
      "\n",
      "fetched sample bytes from nc\n",
      "b'x^\\xbc\\x9b+`\\xf2Z\\xd7\\xad#\\x91\\xc8\\xca\\xc8Hde$\\x12Y\\x19\\x89DVF\"\\x91\\x95\\x91\\x91\\xc8Jd$\\xb222\\x12Yy\\x9eg\\xac\\xd0v\\xef\\xef;\\x17'\n",
      "01111000 01011110 10111100 10011011 00101011 01100000 11110010 01011010 11010111 10101101 00100011 10010001 11001000 11001010 11001000 01001000 01100100 01100101 00100100 00010010 01011001 00011001 10001001 01000100 01010110 01000110 00100010 10010001 10010101 10010001 10010001 11001000 01001010 01100100 00100100 10110010 00110010 00110010 00010010 01011001\n",
      "\n",
      "\n",
      "after decompressing:\n",
      "00001000 00001000 00001000 00001000 00001000 00001000 00001000 00001000 00001000 00001000 \n",
      "\n",
      "after shuffle:\n",
      "00001000 01000000 11000011 01000111 00001000 01000000 11000011 01000111 00001000 01000000 11000011 01000111 00001000 01000000 11000011 01000111 00001000 01000000 11000011 01000111 \n",
      "\n",
      "chunk 1 using 0.0.0 indexing\n",
      "[99968.06 99968.06 99968.06 ... 98759.75 98760.75 98763.5 ]\n"
     ]
    }
   ],
   "source": [
    "# given string of bytes and the json --> recreate the reading/parsing\n",
    "\n",
    "# load meta data into python dict\n",
    "json_path = '/Users/katrinasharonin/Downloads/kerchunkC/jsons/01_air_pressure_at_mean_sea_level.json'\n",
    "f = open(json_path)\n",
    "meta = json.load(f)\n",
    "\n",
    "print('metadata display')\n",
    "print(meta.keys())\n",
    "# print(meta['refs'].keys())\n",
    "# print(len(meta['refs'].keys()))\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"\"\"Return the first n items of the iterable as a list.\"\"\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "n_items = take(10, meta['refs'].items())\n",
    "print('sample dict values')\n",
    "print(n_items[0])\n",
    "print(n_items[1])\n",
    "print(n_items[2])\n",
    "print(n_items[3])\n",
    "print(n_items[4])\n",
    "print(n_items[5])\n",
    "\n",
    "print(meta['refs']['air_pressure_at_mean_sea_level/.zarray'])\n",
    "print(meta['refs']['air_pressure_at_mean_sea_level/0.0.0'])\n",
    "print(meta['refs']['air_pressure_at_mean_sea_level/0.0.1'])\n",
    "print(meta['refs']['air_pressure_at_mean_sea_level/0.0.2'])\n",
    "\n",
    "# compare to actual sub sections from zarr engine - first chunk\n",
    "start_byte = meta['refs']['air_pressure_at_mean_sea_level/0.0.0'][1]\n",
    "num_bytes = meta['refs']['air_pressure_at_mean_sea_level/0.0.0'][2]\n",
    "\n",
    "byte_range = f'bytes={start_byte}-{start_byte+num_bytes}'\n",
    "response = s3.get_object(Bucket=bucket_name, Key=object_key, Range=byte_range)\n",
    "content = response['Body'].read()\n",
    "print('\\n')\n",
    "print('fetched sample bytes from nc')\n",
    "print(content[:50])\n",
    "binary_representation = bytes_to_binary(content[:40])\n",
    "print(binary_representation)\n",
    "\n",
    "buf = zlib.decompress(content)\n",
    "\n",
    "print('\\n')\n",
    "print('after decompressing:')\n",
    "for value in buf[:10]:\n",
    "    print(bin(value)[2:].zfill(8) + \" \", end = '')\n",
    "\n",
    "# use filter + fill value to restore proper buf\n",
    "# filters\":[{\"elementsize\":4,\"id\":\"shuffle\"}],\n",
    "buf = numcodecs.shuffle.Shuffle(4).decode(buf)\n",
    "\n",
    "print('\\n')\n",
    "print('after shuffle:')\n",
    "for value in buf[:20]:\n",
    "    print(bin(value)[2:].zfill(8) + \" \", end = '')\n",
    "\n",
    "chunk = np.frombuffer(buf, dtype='<f4')\n",
    "\n",
    "print('\\n')\n",
    "print('chunk 1 using 0.0.0 indexing')\n",
    "# print(buf)\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare against actual xarray Zarr engine; contents should match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray (variable: 1, time0: 24, lat: 100, lon: 100)>\n",
      "array([[[[ 99968.06,  99968.06,  99968.06, ...,  99968.06,  99968.06,\n",
      "           99968.06],\n",
      "         [ 99949.81,  99950.06,  99950.06, ...,  99965.81,  99965.81,\n",
      "           99966.06],\n",
      "         [ 99925.56,  99925.81,  99926.31, ...,  99958.56,  99958.81,\n",
      "           99959.31],\n",
      "         ...,\n",
      "         [ 99567.56,  99568.81,  99571.81, ..., 100111.81, 100103.06,\n",
      "          100097.56],\n",
      "         [ 99708.31,  99709.56,  99710.56, ..., 100131.56, 100123.81,\n",
      "          100118.31],\n",
      "         [ 99839.56,  99840.81,  99841.56, ..., 100153.56, 100145.56,\n",
      "          100140.06]],\n",
      "\n",
      "        [[100021.31, 100021.31, 100021.31, ..., 100021.31, 100021.31,\n",
      "          100021.31],\n",
      "         [100008.06, 100008.06, 100008.31, ..., 100021.06, 100021.06,\n",
      "          100021.31],\n",
      "         [ 99986.31,  99986.56,  99986.81, ..., 100014.06, 100014.31,\n",
      "          100014.56],\n",
      "...\n",
      "         [ 98630.56,  98661.31,  98692.06, ...,  98561.81,  98563.56,\n",
      "           98567.56],\n",
      "         [ 98662.06,  98693.06,  98725.81, ...,  98651.06,  98651.56,\n",
      "           98655.06],\n",
      "         [ 98679.06,  98715.81,  98750.31, ...,  98751.81,  98752.31,\n",
      "           98754.81]],\n",
      "\n",
      "        [[100905.75, 100905.75, 100905.75, ..., 100905.75, 100905.75,\n",
      "          100905.75],\n",
      "         [100883.25, 100883.  , 100882.75, ..., 100863.75, 100863.5 ,\n",
      "          100863.25],\n",
      "         [100855.5 , 100855.  , 100854.5 , ..., 100816.25, 100815.75,\n",
      "          100815.5 ],\n",
      "         ...,\n",
      "         [ 98295.25,  98324.5 ,  98352.75, ...,  98589.25,  98591.25,\n",
      "           98595.5 ],\n",
      "         [ 98285.5 ,  98314.75,  98346.25, ...,  98668.  ,  98669.  ,\n",
      "           98672.5 ],\n",
      "         [ 98298.75,  98333.75,  98366.75, ...,  98759.75,  98760.75,\n",
      "           98763.5 ]]]], dtype=float32)\n",
      "Coordinates:\n",
      "  * lat       (lat) float32 90.0 89.75 89.5 89.25 89.0 ... 66.0 65.75 65.5 65.25\n",
      "  * lon       (lon) float32 0.0 0.25 0.5 0.75 1.0 ... 24.0 24.25 24.5 24.75\n",
      "  * time0     (time0) datetime64[ns] 2020-01-01 ... 2020-01-01T23:00:00\n",
      "  * variable  (variable) object 'air_pressure_at_mean_sea_level'\n",
      "Attributes:\n",
      "    institution:  ECMWF\n",
      "    source:       Reanalysis\n",
      "    title:        ERA5 forecasts\n",
      "240000\n",
      "should equal...\n",
      "240000\n",
      "[ True  True  True  True  True  True  True  True  True  True]\n",
      "flatten: the target array exists in the 'first_chunk' dataset.\n",
      "match was found in flattened!\n",
      "All search options exhausted, see above for results\n"
     ]
    }
   ],
   "source": [
    "# find the corresponding chunk in the xarray reading w/ zarr engine\n",
    "\n",
    "# contrast against: xarray reading of zarr with engine\n",
    "# chunk sizing: chunks\":[24,100,100]\n",
    "\n",
    "ds = xr.open_dataset(\"reference://\", engine=\"zarr\", backend_kwargs={\n",
    "                    \"consolidated\": False,\n",
    "                    \"storage_options\": {\"fo\": json_path, \"remote_protocol\": \"s3\",\"remote_options\": {\"anon\": True}}\n",
    "                    })\n",
    "\n",
    "# print(ds)\n",
    "\n",
    "first_chunk = ds.isel(time0=slice(0, 24), lat=slice(0, 100), lon=slice(0, 100))\n",
    "print(first_chunk.to_array())\n",
    "print(first_chunk.to_array().size)\n",
    "print('should equal...')\n",
    "print(len(chunk))\n",
    "\n",
    "first_flatten = first_chunk['air_pressure_at_mean_sea_level'].values.flatten()\n",
    "matching_mask = (first_flatten == chunk)\n",
    "\n",
    "# Check if any True values exist in the mask\n",
    "print(matching_mask[:10])\n",
    "if matching_mask.any():\n",
    "    print(\"flatten: the target array exists in the 'first_chunk' dataset.\")\n",
    "else:\n",
    "    print(\"flatten: the target array does not exist in the 'first_chunk' dataset.\")\n",
    "\n",
    "for ex in chunk:\n",
    "    if ex in first_flatten:\n",
    "            print(\"match was found in flattened!\")\n",
    "            break\n",
    "print(\"All search options exhausted, see above for results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99968.06 99968.06 99968.06 ... 98759.75 98760.75 98763.5 ]\n"
     ]
    }
   ],
   "source": [
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test xarray sizing - find variation in sizing\n",
      "<xarray.DataArray (lon: 100)>\n",
      "array([99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06, 99968.06, 99968.06,\n",
      "       99968.06, 99968.06, 99968.06, 99968.06], dtype=float32)\n",
      "Coordinates:\n",
      "    lat       float32 90.0\n",
      "  * lon       (lon) float32 0.0 0.25 0.5 0.75 1.0 ... 24.0 24.25 24.5 24.75\n",
      "    time0     datetime64[ns] 2020-01-01\n",
      "    variable  <U30 'air_pressure_at_mean_sea_level'\n",
      "Attributes:\n",
      "    institution:  ECMWF\n",
      "    source:       Reanalysis\n",
      "    title:        ERA5 forecasts\n",
      "100\n",
      "100\n",
      "24\n",
      "observation; lon aka last is varying on the deepest level\n",
      "observation; time is iterating on the highest level\n"
     ]
    }
   ],
   "source": [
    "print('test xarray sizing - find variation in sizing')\n",
    "print(first_chunk.to_array()[0][0][0])\n",
    "print(len(first_chunk.to_array()[0][0][0]))\n",
    "print(len(first_chunk.to_array()[0][0]))\n",
    "print(len(first_chunk.to_array()[0]))\n",
    "\n",
    "print('observation; lon aka last is varying on the deepest level')\n",
    "print('observation; time is iterating on the highest level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray (lat: 100, lon: 100)>\n",
      "array([[ 99968.06,  99968.06,  99968.06, ...,  99968.06,  99968.06,\n",
      "         99968.06],\n",
      "       [ 99949.81,  99950.06,  99950.06, ...,  99965.81,  99965.81,\n",
      "         99966.06],\n",
      "       [ 99925.56,  99925.81,  99926.31, ...,  99958.56,  99958.81,\n",
      "         99959.31],\n",
      "       ...,\n",
      "       [ 99567.56,  99568.81,  99571.81, ..., 100111.81, 100103.06,\n",
      "        100097.56],\n",
      "       [ 99708.31,  99709.56,  99710.56, ..., 100131.56, 100123.81,\n",
      "        100118.31],\n",
      "       [ 99839.56,  99840.81,  99841.56, ..., 100153.56, 100145.56,\n",
      "        100140.06]], dtype=float32)\n",
      "Coordinates:\n",
      "  * lat       (lat) float32 90.0 89.75 89.5 89.25 89.0 ... 66.0 65.75 65.5 65.25\n",
      "  * lon       (lon) float32 0.0 0.25 0.5 0.75 1.0 ... 24.0 24.25 24.5 24.75\n",
      "    time0     datetime64[ns] 2020-01-01\n",
      "    variable  <U30 'air_pressure_at_mean_sea_level'\n",
      "Attributes:\n",
      "    institution:  ECMWF\n",
      "    source:       Reanalysis\n",
      "    title:        ERA5 forecasts\n"
     ]
    }
   ],
   "source": [
    "print(first_chunk.to_array()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n// temporary compression function\\nstd::vector<Bytef> compressData(Bytef* inputData, \\n                                uLong inputSize, \\n                                int compressionLevel = Z_BEST_COMPRESSION) {\\n    std::vector<Bytef> compressedData;\\n\\n    z_stream stream;\\n    stream.zalloc = Z_NULL;\\n    stream.zfree = Z_NULL;\\n    stream.opaque = Z_NULL;\\n\\n    int result = deflateInit(&stream, compressionLevel);\\n    if (result != Z_OK) {\\n        return compressedData;\\n    }\\n\\n    stream.avail_in = static_cast<uInt>(inputSize);\\n    stream.next_in = const_cast<Bytef*>(inputData);\\n    \\n    uLong outputBufferSize = deflateBound(&stream, inputSize);\\n    compressedData.resize(outputBufferSize);\\n    stream.avail_out = static_cast<uInt>(outputBufferSize);\\n    stream.next_out = &compressedData[0];\\n\\n    result = deflate(&stream, Z_FINISH);\\n    if (result != Z_STREAM_END) {\\n        deflateEnd(&stream);\\n        return compressedData;\\n    }\\n\\n    deflateEnd(&stream);\\n\\n    compressedData.resize(outputBufferSize - stream.avail_out);\\n\\n    std::cout << std::endl;\\n    std::cout << \"Compressed Data (Binary): \";\\n    for (size_t i = 0; i < 100 && i < compressedData.size(); ++i) {\\n        Bytef byte = compressedData[i];\\n        // std::cout << \"\\\\x\";\\n        for (int bit = 7; bit >= 0; --bit) {\\n            std::cout << ((byte >> bit) & 1);\\n        }\\n        std::cout << \" \";\\n    }\\n    std::cout << std::dec << std::endl;\\n\\n    return compressedData;\\n}\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spare code - compression recreation for zlib\n",
    "\n",
    "\"\"\"\n",
    "// temporary compression function\n",
    "std::vector<Bytef> compressData(Bytef* inputData, \n",
    "                                uLong inputSize, \n",
    "                                int compressionLevel = Z_BEST_COMPRESSION) {\n",
    "    std::vector<Bytef> compressedData;\n",
    "\n",
    "    z_stream stream;\n",
    "    stream.zalloc = Z_NULL;\n",
    "    stream.zfree = Z_NULL;\n",
    "    stream.opaque = Z_NULL;\n",
    "\n",
    "    int result = deflateInit(&stream, compressionLevel);\n",
    "    if (result != Z_OK) {\n",
    "        return compressedData;\n",
    "    }\n",
    "\n",
    "    stream.avail_in = static_cast<uInt>(inputSize);\n",
    "    stream.next_in = const_cast<Bytef*>(inputData);\n",
    "    \n",
    "    uLong outputBufferSize = deflateBound(&stream, inputSize);\n",
    "    compressedData.resize(outputBufferSize);\n",
    "    stream.avail_out = static_cast<uInt>(outputBufferSize);\n",
    "    stream.next_out = &compressedData[0];\n",
    "\n",
    "    result = deflate(&stream, Z_FINISH);\n",
    "    if (result != Z_STREAM_END) {\n",
    "        deflateEnd(&stream);\n",
    "        return compressedData;\n",
    "    }\n",
    "\n",
    "    deflateEnd(&stream);\n",
    "\n",
    "    compressedData.resize(outputBufferSize - stream.avail_out);\n",
    "\n",
    "    std::cout << std::endl;\n",
    "    std::cout << \"Compressed Data (Binary): \";\n",
    "    for (size_t i = 0; i < 100 && i < compressedData.size(); ++i) {\n",
    "        Bytef byte = compressedData[i];\n",
    "        // std::cout << \"\\\\x\";\n",
    "        for (int bit = 7; bit >= 0; --bit) {\n",
    "            std::cout << ((byte >> bit) & 1);\n",
    "        }\n",
    "        std::cout << \" \";\n",
    "    }\n",
    "    std::cout << std::dec << std::endl;\n",
    "\n",
    "    return compressedData;\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
